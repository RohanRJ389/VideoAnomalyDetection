{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\pes12/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "c:\\Users\\pes12\\anaconda3\\envs\\env1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5  2024-10-2 Python-3.12.2 torch-2.2.2 CUDA:0 (NVIDIA GeForce MX350, 2048MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample_frame.jpg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Load YOLOv5n model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Get bounding boxes, confidence scores, and class names\n",
    "results_df = results.pandas().xyxy[0]  # Get results as a Pandas DataFrame\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "for index, row in results_df.iterrows():\n",
    "    x1, y1, x2, y2, conf, cls = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax']), row['confidence'], row['name']\n",
    "    label = f'{cls} {conf:.2f}'  # Class name and confidence\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw rectangle\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)  # Add label text\n",
    "\n",
    "# Save the output image\n",
    "output_image_path = 'output_image.jpg'\n",
    "cv2.imwrite(output_image_path, img)\n",
    "\n",
    "# Display the output image in the Jupyter notebook\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = plt.imread(output_image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Remove axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output (before post-processing):\n",
      "tensor([[2.51584e+00, 4.97705e+00, 7.38330e+00,  ..., 2.39702e-03, 1.28765e-03, 3.53809e-03],\n",
      "        [1.31926e+01, 3.96431e+00, 2.29371e+01,  ..., 3.56491e-03, 1.52568e-03, 4.26607e-03],\n",
      "        [1.89548e+01, 3.51026e+00, 2.93802e+01,  ..., 3.77357e-03, 1.66182e-03, 5.68593e-03],\n",
      "        ...,\n",
      "        [5.62466e+02, 5.99950e+02, 1.72303e+02,  ..., 3.26404e-03, 8.99655e-04, 1.21293e-03],\n",
      "        [5.84729e+02, 6.02651e+02, 1.22408e+02,  ..., 2.94512e-03, 9.42764e-04, 1.28025e-03],\n",
      "        [6.20365e+02, 6.11995e+02, 1.61157e+02,  ..., 2.66731e-03, 1.04668e-03, 1.49785e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model (make sure to have YOLOv5 repo cloned or installed)\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Disable the post-processing (NMS, confidence thresholding, etc.)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to preprocess the image for YOLOv5\n",
    "def preprocess_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to RGB format (if not already)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    # Resize the image to (640, 640) as expected by YOLOv5\n",
    "    img = np.array(img)\n",
    "    img = cv2.resize(img, (640, 640))\n",
    "\n",
    "    # Convert image to tensor and normalize (YOLOv5 expects values in [0,1])\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0  # Normalize to 0-1\n",
    "    img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    return img\n",
    "\n",
    "# Path to the input image\n",
    "image_path = 'sample_frame.jpg'  # Replace with your image path\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess_image(image_path)\n",
    "\n",
    "# Forward pass through the model to get raw outputs (without post-processing)\n",
    "with torch.no_grad():\n",
    "    raw_output = model(input_tensor)  # This returns a list with raw output tensors\n",
    "\n",
    "# Print the raw output vector (unprocessed)\n",
    "print(\"Raw output (before post-processing):\")\n",
    "print(raw_output[0])  # Access the tensor from the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 25200, 85]), torch.Size([25200, 85]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output.shape,raw_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
